# BREAST CANCER ANALYSIS

Along this Rnotebook we will analyze the features obtained from the [Breast Cancer's Dataset](https://archive.ics.uci.edu/dataset/17/breast+cancer+wisconsin+diagnostic). We will apply the methods and techniques seen during the course of Multivariate Statistics of the University of Granada.

## **1. Loading packages and data sets**

### 1.1 Loading and installing R packages

```{r warning=FALSE, message=FALSE}

#########################################
# Loading necessary packages and reason #
#########################################

# This is an example of the first installation of a package
# Only runs once if the package is not installed
# Once it is installed this sentence has to be commented (not to run again)
#install.packages("ggplot2")
#install.packages("summarytools")
#install.packages("tidyverse")
#install.packages("pastecs")

# Julián' problems: I had to run these commands on my ubuntu terminal and also install additional packages to make the doc works.
  # sudo apt-get install r-base-dev libcurl4-openssl-dev libxml2-dev libssl-dev libmagick++-6.q16-dev
  # sudo apt-get install libharfbuzz-dev
  # sudo apt-get install libfribidi-dev
  #
  #install.packages("ggpubr")
  #install.packages("biotools")
  #install.packages("caret")
  #install.packages("klaR")
  # and others... (i chose an option to install all dependencies automatically)
  
  # Also i had to install GSL and gsl package to install and use MNV library:
  # sudo apt-get install libgsl-dev
  #install.packages("mnv")



# Package required to call 'freq' and 'descr' functions (descriptive statistics)
library(summarytools)

# Package required to call 'ggplot' function
library(ggplot2)

# Package required to call 'ggarrange' function
library(ggpubr)

# Package required to call 'scatterplot3d' function
library(scatterplot3d)

# Package required to call 'melt' function
library(reshape2)

# Package required to call 'mvn' function
library(MVN)

# Package required to call 'boxM' function
library(biotools)

# Package required to call 'summarise' function
library(dplyr)

# Package required to call 'createDataPartition' function
library(caret)

# Package required to call 'read.spss' function (loading '.spss' data format)
library(foreign)

# Package required to call 'read_xlsx' function (loading '.xlsx' data format)
library(readxl)

# Package required to load the data set 'RBGlass1'
library(archdata)

# Package required to call 'cortest.bartlett' function
library(psych)

# Package required to call 'fviz_pca_var, fviz_pca_ind and fviz_pca' functions
library(factoextra)

# Package required to call 'scatterplot3d' function
library(scatterplot3d)

# Package required to call 'factanal' function
library(stats)

# Package required to call 'cortest.bartlett' function
library(psych)

# Package required to call 'hetcor' function
library(polycor)

# Package required to call 'ggcorrplot' function
library(ggcorrplot)

# Package required to call 'corrplot' function
library(corrplot)

# Package required to call 'rplot' function
library(corrr)

library(tidyverse)

# Package required to call 'partimat' function
library(klaR)

# Package required to call 'clusGap' function
library(cluster)

# Package required to call 'ggdendrogram' function
library(ggdendro)

# Package required to call 'grid.arrange' function
library(gridExtra)

```

### 1.2 Loading data set

```{r}

#file path

#Valentin's path
#setwd("C:/Users/valentin/Desktop/DGIIM 5/1 CUATRI/MULTIVARIANTE/PRACTICAS/PRACTICA_FINAL/Breast_Cancer_Analysis")

#Javi's path
#setwd("/home/javi5454/Desktop/Github/Breast_Cancer_Analysis")

#Julián's path
getwd()
setwd("/home/julian/Escritorio/5_DGIIM/Est_Mult/PRACTICAS/P_FINAL/Breast_Cancer_Analysis")

#loading data. We haven seen that the file has the values of each feature separated by "," so we use as argument the specific separator ",". The file does not have header so we also specify it.
dataframe <- read.table("wdbc.data", header = FALSE, sep=",")

#verifying data.
head(dataframe)
```

In the previous table we can see that the features has not the right name, even, in the table there are some feature that we are not interesting in our study, as they are used in machine learning's context to make predictions and not in multivariate analysis.

In the following cells we will preprocess the table to erase these columns, and to rename the features correctly, before starting with any research of our data.

```{r}

#Columns' names from the dataset description
colnames<-c('ID','DIAGNOSIS','radius1', 'texture1','perimeter1','area1','smoothness1','compactness1','concavity1','concave_points1','symmetry1','fractal_dimension1','radius2','texture2','perimeter2','area2','smoothness2','compactness2','concavity2','concave_points2','symmetry2','fractal_dimension2','radius3','texture3','perimeter3','area3','smoothness3','compactness3','concavity3','concave_points3','symmetry3','fractal_dimension3')

#changing columns' names
colnames(dataframe)<- colnames

#erasing the first two columns 
datos <- dataframe[, -c(1, 2)]

#finally we add an extra column to our dataframe which funtion is just identify each row, but regarding our order and not the dataframe's one.
datos$ID <- 1:nrow(datos)
datos <- datos[, c('ID', names(datos)[-ncol(datos)])]

#showing the data to verify the changes
datos
```

```{r}
#discomment in case the file with the preprocessed dataframe is needed.
#write.table(datos, file = 'dataframe.csv', sep = ',', row.names = FALSE)
```

### 1.3 Data Base description

The file *wdbc.data* contains data collected from 569 patients based on 10 different indicators of breast cancer. We have 3 different measures for each indicator, each one corresponding to a spacial dimensions (XYZ axis):

-   Radius (mean of distances from center to points on the perimeter).

-   Texture (standard deviation of gray-scale values).

-   Perimeter.

-   Area.

-   Smoothness (local variation in radius lengths).

-   Compactness ($\frac{perimeter^2}{area - 1.0}$).

-   Concavity (severity of concave portions on the contour).

-   Concave points (number of concave portions of the contour).

-   Symmetry.

-   Fractal dimension ("coastline approximation" - 1).

## 2. Univariate Exploratory Analysis

Firstly, we would like to give an brief description of the data, to be more aware of what types of data we are working on.

The public dataset we are working on is related to the Breast Cancer. All the values given are computed from digitized images of a fine needle aspirate (FNA) of a breast mass. They describe **characteristics of the cell nucleus**.

The dataframe consists on 30 continuous and numeric features (plus the ID and Target one). Each feature is actually a real-value computed for each cell nucleus.

In our opinion, we belief that this dataset is clearly a dataset to perform Machine Learning, as they even included one Target feature. The features are intuitive and the idea behind the dataset is also clear and concise; predict the type of Breast Cancer (malignant or bening).

So, after this quick analysis of the dataset we can start to perform the Univariate Exploratory Analysis.

### 2.1 Data grouping or recoding

Regarding the types of values we have on the dataset and the range of the domain where the features take their values, we are not interested on perform data-grouping or to recode them.

### 2.2 Missing values

Another key part of the Univariate Exploratory Analysis is to identify missing values. On the following cells we are going to compute the porcentage of missing values of each feature. ![]()

```{r}
# compute the missing values for each feature of our dataset (We don't have to do this because our dataset doesn't have missing values)
# missing_values <- sapply(datos, function(x) sum(is.na(x))/length(x) * 100)

# we plot the results of the missing values data 
barplot(missing_values, main = "Porcentage of missing values", xlab = "", ylab = "Porcentage of missing values", col = "steelblue", las = 2, ylim = c(0, 100))

```

In the plot we can see clearly that **there are not missing values.** We are not need in our case to study the random pattern of the missing values.

If we would have obtained more than 5% of missing values on any feature, we would have to analyze the random patterns. As we have continuous variables we would have needed to apply a Student test. Regarding the results of the applied test we would have obtained that the pattern is random or not, and in both cases, replacing the missing values by the mean or median.

### 2.3 Classical numeric descriptive analysis

Following, we are going to compute basic statistics measures to gather more information about or data. As the variables are quantitative, we are going to compute **basic numerical descriptive statistics, their histograms, density and boxplot.**

```{r}
# for every feature except the ID
for(col in names(datos)[-1]) {

  cat("\nSummary of the feature: ", col, "\n")
  
  # showing the summary
  print(summary(datos[[col]]))
}

```

As the function we have used to display this values is maybe too simple, we are going to display more measures about our features to understand better our dataset.

```{r}
#compute the measures for all features except the ID
breast_cancer_dataset <- datos[, -1]
descr_res <- descr(breast_cancer_dataset)

#show adequately the table
print((descr_res), method = 'render')
```

Next, we will plot some interesting graphs of each feature, such as density function, histogram and boxplot.

```{r}

for(col in names(datos)[-1]) {

  # creating density, histogram and boxplot
  p1 <- ggplot(datos[-1], aes_string(x = col)) + geom_density() +
    labs(title = paste("Density function of", col), x = col, y = "Values") +
    theme(plot.title = element_text(size = 8, hjust = 0.5))
  
  p2 <- ggplot(datos[-1], aes_string(x = col)) + geom_histogram(bins = 30) +
    labs(title = paste("Histogram of", col), x = col, y = "Values") +
    theme(plot.title = element_text(size = 8, hjust = 0.5))
  
  p3 <- ggplot(datos[-1], aes_string(x = col)) +
    geom_boxplot(outlier.colour = "red", outlier.shape = 1, outlier.size = 2) +
    coord_flip() + labs(title = paste("Boxplot of", col), x = "Values", y = "") +
    theme(plot.title = element_text(size = 8, hjust = 0.5))
  
  # show the graphs
  print(ggarrange(p1, p2, p3, nrow = 1, common.legend = FALSE))
}


```

Just as an addition to all information gathered, we are interested on seeing the Target feature of the dataset, "Diagnosis". Therefore, we will show some graphical outputs about this categorical variable.

```{r}
# frequency tables
freq(dataframe[2])
```

Just as a reminder, B = benign and M = malignant.

```{r}

# create the pie chart and bar graph for the Target column (DIAGNOSIS)
p1 <- ggplot(dataframe, aes(x = factor(1), fill = DIAGNOSIS)) + geom_bar() +
  coord_polar("y") + labs(x = "DIAGNOSIS", y = "%")
  
p2 <- ggplot(dataframe, aes(x = factor(1), fill = DIAGNOSIS)) + geom_bar() +
  labs(x = "DIAGNOSIS", y = "%")

# display the graphs
print(ggarrange(p1, p2, nrow = 1, ncol = 2, common.legend = TRUE))
```

### 2.4 Outliers

In this section we will try to identify outliers in each feature as some methods we will use along this proyect are sensitive to the presence of outliers.

We have to remind that all our features are numerical variables so we can apply some graphs methods to identify intuitively the outliers. It is remarkable that firstly we standarize the data because, as we can see in the previous sections, our features have different scales.

```{r}

# standarize
sca <- scale(datos[-1])

# transform data to large format
datos_largos <- as.data.frame(sca) %>% 
  rownames_to_column(var = "ID") %>% 
  pivot_longer(cols = -ID, names_to = "Variable", values_to = "Valor")

# reformat the graphical options
options(repr.plot.width = 10, repr.plot.height = 8)

# create boxplot
p <- ggplot(datos_largos, aes(x = Variable, y = Valor, fill = Variable)) +
  geom_boxplot() +
  theme(axis.text.x = element_text(angle = 90, hjust = 1)) +
  labs(title = "Outliers", x = "All explanatory variables", y = "Values") +
  scale_fill_discrete(name = "Variables") 

# display the graphs
print(p)

```

Another way of outliers' visualization:

```{r}

columnas <- colnames(datos[-1])

# boxplot for each feature
for (col in columnas) {
  p <- ggplot(datos[-1], aes_string(x = col)) +
    geom_boxplot(outlier.colour = "red", outlier.shape = 1, outlier.size = 2) +
    coord_flip() +
    labs(title = paste("Boxplot of", col), x = "Values", y = "")
  
  # show the graph
  print(p)
}

```

In both graphs we can see that there are outliers in almost every feature. Regarding this, we can eliminate them or replace by the mean or median. As this is not rigurous we are going to take the one option who give us better results.

```{r}
# ---   OUTLIERS METHODOLOGY ---

# replacing outliers by the mean
reemplazar_outliers <- function(x) {
  qnt <- quantile(x, probs=c(.25, .75), na.rm = TRUE)
  H <- 1.5 * IQR(x, na.rm = TRUE)
  outliers <- x < (qnt[1] - H) | x > (qnt[2] + H)
  x[outliers] <- mean(x[!outliers])
  return(x)
}



# we apply the function to every feature of our dataset
datos_preprocessed <- datos %>% mutate(across(-1, reemplazar_outliers))


```

```{r}
# Uncomment to undo the outliers treatment
# datos_preprocessed = datos
```

We have created a function that erase the outliers, just taking in count the quantiles of each feature and leaving the values which are not considered as outliers.

We will replot the boxplot of each feature with the changes.

```{r}
columnas <- colnames(datos_preprocessed[-1])

# boxplot for each feature
for (col in columnas) {
  p <- ggplot(datos_preprocessed[-1], aes_string(x = col)) +
    geom_boxplot(outlier.colour = "red", outlier.shape = 1, outlier.size = 2) +
    coord_flip() +
    labs(title = paste("Boxplot of", col," (without outliers)"), x = "Values", y = "")
  
  # show the graph
  print(p)
}
```

### 2.5 Univariate normality

As many techniques can not avoid the assumption of normality is necessary to check the distribution of each feature of the dataset. In order to analyze their distribution we are going to use two techniques. The first one is graphical, using the funcion qqplot, and the second one is about applying the Univariate normality test (Shapiro-Wilk).

```{r}
columnas <- colnames(datos_preprocessed[-1])

# creating qqplot
for (col in columnas) {
  
  p <- ggplot(data = datos_preprocessed, aes(sample = get(col))) +
    stat_qq(distribution = qnorm, dparams = list(mean = mean(datos_preprocessed[[col]], na.rm = TRUE), sd = sd(datos_preprocessed[[col]], na.rm = TRUE))) +
    geom_abline(color = "red", linetype = "dashed") +  # Agrega una línea diagonal
    ggtitle(paste("Q-Q plot of", col)) +
    xlab("Theoretical Quantiles") +
    ylab("Sample Quantiles")
  
  print(p)
}


```

```{r}

columnas <- names(datos_preprocessed)[-1]

# Crea un histograma para cada característica
for (j0 in columnas) {
  # Genera el histograma
  p <- ggplot(datos_preprocessed, aes_string(x = j0)) +
    geom_histogram(aes(y = ..density..), bins = 30, fill = "lightblue", color = "black") +
    geom_density(color = "red", lwd = 1) +
    labs(title = paste("Histogram of", j0), x = j0, y = "Density")
  
  # Muestra el histograma
  print(p)
}

```

With this exploratory analysis we can have a general idea of the possible normal distribution of the univariate variables. However, as these are graphical outputs and the test is just our "sight" we will apply some more mathematical and numerical test to accept the assumption of normality.

The null hypothesis of the test is that the data **follow a univariate normal distribution.** This hypothesis is rejected if the p-value given by the Shapiro-Wilk test is less than 0.05.

```{r}
#reformat the data
datos_tidy <- melt(datos_preprocessed, id.vars = "ID", variable.name = "variable", value.name = "value")

# Shapiro-Wilk test for each variable.
resultados <- aggregate(value ~ variable, data = datos_tidy, FUN = function(x){shapiro.test(x)$p.value})

# new column to find if the value obtained is greater than the p-value
resultados$NORMALITY <- ifelse(resultados$value < 0.05, "NO", "YES")

# display results
print(resultados)

```

Looking on the results obtained we can see that despite we obtained in the graphical outputs that the most part of the variables followed a normal distribution, actually they do not follow a normal distribution. Thanks to the Shapiro-Wilk test we were able to prove it, and it is a important test always to contrast what "we think" with the mathematically right results.

## 3. Multivariate Exploratory Analysis

### 3.1 Correlation

Firstly, it is necessary to check if the variables are or not independent. We could do it observing the correlation matrix, at sample level. At population level, we could check if there is correlation using Bartlett's Test (this test is used to test the null hypothesis, $H_0$ that all $k$ population variances are equal against the alternative that at least two are different).

```{r}
###############################
# Correlation at sample level #
###############################

# Are the variables correlated at sample level?
correlation_matrix<-cor(datos_preprocessed[-1])
correlation_matrix
```

```{r}
det(correlation_matrix)
```

It is noticed an important correlation between some variables. For example, we can see it between perimeter1 and area:

```{r}
cor(datos_preprocessed$perimeter1, datos_preprocessed$area1)
```

Let's study correlation at population level

```{r}
###################################
# Correlation at population level #
###################################

# Bartlett's sphericity test:
# This test checks wheter the correlations are significantly different from 0
# The null hypothesis is H_0; det(R)=1 means the variables are uncorrelated
# R denotes the correlation matrix
# cortest.bartlett function in the package pysch performs this test
# This function works with standarized data

#Standardization
datos_preprocessed_scale<-scale(datos_preprocessed[-1])

# Bartlett's sphericity test
cortest.bartlett(cor(datos_preprocessed_scale), 569) # 569 is the sample size
```

Due to $p-value$ is 0\<0.001, we can assume that there is no independence, so it is logical to propose dimension's reduction trough a Principal Component Analysis or Factorial Analysis.

### 3.2 Principal Components Analysis (PCA)

The following code performs the PCA, obtaining the eigenvectors of each component and them eigenvalues (variance of each one).

```{r}
# The 'prcomp' function in the base R package performs this analysis
# Parameters 'scale' and 'center' are set to TRUE to consider standardized data
PCA<-prcomp(datos_preprocessed[-1], scale=T, center=T)

# The field 'rotation0 of the 'PCA' object is a matrix
# Its columns are the coefficients of the principal components
# Indicates the weight of each variable in the corresponding principal component
PCA$rotation
```

At field "sdev" PCA object we can find standard deviations of each principal component. With summary function applied to PCA object, we obtain some interesting measures:

```{r}
PCA$sdev
```

```{r}
summary(PCA)
```

The following graph shows the proportion of explained variance

```{r}
explained_variance<-PCA$sdev^2 / sum(PCA$sdev^2)

p1<-ggplot(data=data.frame(explained_variance, pc=1:ncol(datos_preprocessed[-1])), 
           aes(x= pc, y = explained_variance, fill=explained_variance)) + geom_col(width = 0.3) +
           scale_y_continuous(limits = c(0,0.6)) + theme_bw() + labs(x = "Principal component", y = "Proportion of variance")

p1
```

The following graph shows the proportion of cumulative explained variance

```{r}
cummulative_variance<-cumsum(explained_variance)

p2<-ggplot(data = data.frame(cummulative_variance, pc= 1:ncol(datos_preprocessed[-1])), aes(x = pc, y = cummulative_variance, fill=cummulative_variance)) + geom_col(width = 0.5) + scale_y_continuous(limits = c(0,1)) + theme_bw() + labs(x = "Principal component", y = "Proportion of cumulative variance")

p2
```

### 3.3 Appropriate number of principal components

There are different methods to carry out this process, but we will use Rule of Abdi et al. (2010): The variances explained by the principal components are averaged and those whose proportion of explained variance exceeds the mean are selected.

```{r}
PCA$sdev^2
```

```{r}
mean(PCA$sdev^2)
```

```{r}
counter<-1
print("Prinicipal components considered:")
for(element in PCA$sdev^2){
  if(element > mean(PCA$sdev^2)){
    print(colnames(PCA$x)[counter])
  }
  counter<-counter + 1
}
```

So, we would only consider the first 6th components. They accumulate near to 82% of the explained variance. Each principal component is obtained as linear combination of all components with the coefficients indicated at the columns of rotation matrix.

There is the possibility of representing different pairwise comparisons between the principal components, but since we have 6 principal components, we would have to create $\frac{6!}{2} = 360$ graphs, which is impractical. Instead, we will only make the comparison graphs between first and second principal component, which are in fact, the two ones with most part of the explicable variance.

Firstly, the following graphical outputs show the projection of the variables in two dimensions. Display the weight of the variable in the direction of the principal component.

```{r}
p1<-fviz_pca_var(PCA,repel = T, col.var = "cos2", legend.title="Distance", title="Variables")+theme_bw()

p1
```

It is also possible to represent the observations. As well as identify with colors those observations that explain the greatest variance of the principal components.

```{r}
p1<-fviz_pca_ind(PCA, col.ind = "contrib", gradient.cols = c("#00AFBB", "#E7B800", "#FC4E07"), 
                 repel = T, legend.title = "Contrib.var", title ="Recrods") + theme_bw()

p1
```

Now, joint representation of variables and observations. Relates the possible relationships between the contributions of the records to the variances of the components and the weight of the variables in each principal component.

```{r}
p1<-fviz_pca(PCA, alpha.ind = "contrib", col.var="cos2", col.ind="seagreen", gradient.cols = c("#FDF50E", "#FD960E", "#FD1E0E"), repel = T, legend.title = "Distancia") + theme_bw()

p1
```

### 3.4 Factorial Analysis

First of all in this section, different graphical outputs are illustrated below that provide an intuitive idea of correlation between the variables. A *trained eye* could anticipate the appropriate number of factor with this visual information.

```{r}
# Polychoric correlation matrix
poly_cor<-hetcor(datos_preprocessed[-1])$correlations

ggcorrplot(poly_cor, type="lower", hc.order = T)
```

Another interesting visual representation is the following:

```{r}
corrplot(cor(datos_preprocessed[-1]), order="hclust", tl.col="black", tl.cex = 1)
```

One last representation is the following:

```{r}
datos_preprocessed_correlations<-correlate(datos_preprocessed[-1]) #Correlations object
rplot(datos_preprocessed_correlations, legend = T, colours = c("firebrick1", "black", "darkcyan"), print_cor = T)
```

Observing the graphical outputs, it seems to be between 3 and 4 groups for latent variables with high correlation with a group of observable variables and low correlation with the others. It can help us to decide which is the optimum number of factor for our Factorial Analysis.

Now, we must choose a method to extract the factors: main factor, likelihood, etc. First, we will compare the outputs of the maximum likelihood method and minimal residual model. We will calculate 3 factors due to the last graphs.

```{r}
### Test of two models with three factors
model1<-fa(poly_cor, nfactors = 3, rotate = "none", fm="mle") #likhelihood method

model2<-fa(poly_cor, nfactors = 3, rotate = "none", fm = "minres") #minimal residual model

# Outputs of these models: factorial matrices, etc.

print("Model 1: maximum likelihood esthimator")
model1$loadings

print("Model 2: minimal residual model")
model2$loadings
```

Now, a comparison of the communalities of these two methods is illustrated. It appears that communalities of the likelihood model are lower than those of the minimum residual model.

```{r}
# Comparing communalities
sort(model1$communality, decreasing = T)->c1
sort(model2$communality, decreasing = T)->c2

cbind(c1,c2)
```

A comparison of uniqueness is also performed. That is the proportion of variance that has not been explained by the factor (1 - communality)

```{r}
sort(model1$uniquenesses, decreasing = T)->u1
sort(model2$uniquenesses, decreasing = T)->u2
cbind(u1,u2)
```

We can observe that with the minimal residual model we obtain higher values, then latent factors obtained with these methods explains better the variance of observed variables.

### 3.5 Choosing the optimal number of factors to consider

There are different criteria, among which the Scree plot and parallel analysis stand out. The **Scree plot** consist in represent the scree plot obtained from the representation of eigenvalues in descendant order at Y-axis and the number of latent factors at X-axis. Joining the points we obtain a figure that starts with a strong drop and then a leveling-off. The methods indicates that we have to choose the number of factors where the graphical output makes an "elbow". The parallel analysis is a technique designed to help take some of the subjectivity out of interpreting the scree plot. Graphically, it identifies the point where the cumulative explained variance of real data significantly surpasses the average cumulative explained variance of random data, determining the optimal number of factors to retain. This method helps prevent overextraction of factors.

```{r}
# Scree plot
scree(poly_cor)
```

```{r}
#Parallel anaylisis
fa.parallel(poly_cor, n.obs=100, fa="fa", fm="minres")
```

In this case, we will chose 5 factors due to parallel analysis.

Other way to study the optimal number of factors is with the following hypothesis test, which contrast if the number of factors is enough or not.

```{r}
factanal(datos, factors=5, rotation="none", lower=0.02)
```

So, we do not reject the null hypothesis, and we will use 5 factors.

Finally, we will make an estimation of the factorial model with 5 factors. We implement a varimax rotation to seek a simpler interpretation.

```{r}
varimax_model<-fa(poly_cor, nfactors=5, rotate="varimax", fa="minres")
```

We obtain a warning telling us an ultra-Heywood case was detected. Since communalities are squared correlations, you would expect them always to lie between 0 and 1. It is a mathematical peculiarity of the common factor model, however, that final communality estimates might exceed 1. If a communality exceeds 1, it is an ultra-Heywood case. It implies that some unique factor has negative variance, a clear indication that something is wrong. One possible cause is that we are using too much factors. Let's try with 3 factors:

```{r}
varimax_model<-fa(poly_cor, nfactors=3, rotate="varimax", fa="minres")
```

Here we do not get any warning of an ultra-Heywood case. We will run again our previous hypothesis test, but now with three factors.

```{r}
factanal(datos, factors=3, rotation="none", lower=0.03)
```

So, then we will use 3 factors in our model. Now, the rotated factorial matrix is shown.

```{r}
print(varimax_model$loadings, cut = 0)
```

Visually we could make the effort to see what variables each correlates with one of the factors, but it is very tedious. So we use the following representation in diagram mode.

```{r}
fa.diagram(varimax_model)
```

### 3.6 Multivariate Normality of the data

As the multivariate normality can be affected by the presence of multivariate outliers we will plot again the outliers in relation with the Chi-Square qqplot.

```{r}
outliers <- mvn(data = datos[,-1], mvnTest = "hz", multivariateOutlierMethod = "quan")
```

Therefore, as the number of outliers is enough small, we can perfom the following tests in order to find some evidence of lack of multivariate normality.

```{r}
royston_test <- mvn(data = datos_preprocessed[,-1], mvnTest = "royston", multivariatePlot = "qq")
royston_test$multivariateNormality

```

```{r}
hz_test <- mvn(data = datos_preprocessed[,-1], mvnTest = "hz")
hz_test$multivariateNormality
```

## **4.Linear Discriminant Analysis (LDA)**

```{r}
#We create a new variable that includes the dataframe with our ID system and also the categorical variable "DIAGNOSIS" which one wasn't used before and therefore erased in "datos" variable
datos2 <- dataframe[-1]
datos2$ID <- 1:nrow(datos2)
datos2 <- datos2[, c('ID', names(datos2)[-ncol(datos2)])]
datos2
```

Considering now this qualitative variable "DIAGNOSIS", it is interesting to build a classifier to distinct between its two categories: 
  - M: Denotes "Malignant" 
  - B: Denotes "Benign"
In order to to classify cancer impact and patients' health. So Diagnosis would be or response variable and the other ones except ID would be the explanatory variables.
```{r}
# The response variable has to be an object of the class 'factor' of R language
datos2$DIAGNOSIS<-as.factor(datos2$DIAGNOSIS)
```
### 4.1 Graphical exploration of data

First, we explore how well (or poorly) each of the explanatory variables considered independently classifies the Diagnosis.
```{r}
diagnosis <- datos2$DIAGNOSIS

# Create a list to store the graphics
g_list <- list()

# Names of the columns we want to graph
explanatory <- names(datos2[, -c(1, 2)])

# For loop to create the graphics and store them in the list
for (col in explanatory) {
  plot <- ggplot(data = datos2[-1], aes_string(x = col, fill = diagnosis)) +
          geom_histogram(position = "identity", alpha = 0.5)
  
  print(plot)
  #g_list[[paste0("p", length(g_list) + 1)]] <- plot
}
#ggarrange(plotlist=g_list, nrow = length(g_list), common.legend = TRUE)
# ggarrange(g_list[[1]], g_list[[2]], g_list[[3]], #g_list[[4]], g_list[[5]], g_list[[6]], g_list[[7]], g_list[[8]], g_list[[9]], g_list[[10]],
#           #g_list[[11]], g_list[[12]], g_list[[13]], g_list[[14]], g_list[[15]], g_list[[16]], g_list[[17]], g_list[[18]], g_list[[19]], g_list[[20]],
#           #g_list[[21]], g_list[[22]], g_list[[23]], g_list[[24]], g_list[[25]], g_list[[26]], g_list[[27]], g_list[[28]], g_list[[29]], g_list[[30]],
#           nrow=3, common.legend = TRUE)
```

# ```{r}
# ggarrange(g_list[[1]], g_list[[2]], g_list[[3]], nrow=3, common.legend = TRUE)
# ```
# ```{r}
# ggarrange(g_list[[4]], g_list[[5]], g_list[[6]], nrow=3, common.legend = TRUE)
# ```
# ```{r}
# ggarrange(g_list[[7]], g_list[[8]], g_list[[9]], nrow=3, common.legend = TRUE)
# ```
# ```{r}
# ggarrange(g_list[[10]], g_list[[11]], g_list[[12]], nrow=3, common.legend = TRUE)
# ```
# ```{r}
# ggarrange(g_list[[13]], g_list[[14]], g_list[[15]], nrow=3, common.legend = TRUE)
# ```
# ```{r}
# ggarrange( g_list[[16]], g_list[[17]], g_list[[18]], nrow=3, common.legend = TRUE,heights = c(5,5,5))
# ```
# ```{r}
# ggarrange(g_list[[19]], g_list[[20]], g_list[[21]], nrow=3, common.legend = TRUE)
# ```
# ```{r}
# ggarrange(g_list[[22]], g_list[[23]], g_list[[24]], nrow=3, common.legend = TRUE)
# ```
# ```{r}
# ggarrange(g_list[[25]],g_list[[26]], g_list[[27]], nrow=3, common.legend = TRUE)
# ```
# ```{r}
# ggarrange(g_list[[28]], g_list[[29]], g_list[[30]], nrow=3, common.legend = TRUE)
#```

We could see that area1, concave_points1, radius3, area3, perimeter3 and concave_points3 are the best options to differentiate between benign and malignant.
Next, we could think about exploring which pairs of variables best separate between diagnosis but due to the high number of explanatory variables we have, this technique would generate too many combinations and that's not worthy. However, we add the commented code to give the possibility of visualizing graphics of pairs of variables for each axis (variables grouped 10 by 10).
```{r}
# exp1<-explanatory[1:10]
# exp2<-explanatory[11:20]
# exp3<-explanatory[21:30]
# 
# # Margins adjust
# par(mar = c(0.1, 0.1, 0.1, 0.1))
# 
# # Create a vector for different points representations
# pch_vector <- ifelse(datos2$DIAGNOSIS == "M", 4, 1)  # Usar pch = 4 para Diagnosis M, pch = 1 para Diagnosis B
# 
# # Create pairs graphic -> Change exp"number" to visualize the different groups of variables
# pairs(x = (datos2[-1])[, exp1], col = c("orange", "blue")[datos2$DIAGNOSIS], pch = pch_vector)
# 
# # Adding the legend - (not working)
# #legend("topright", legend = unique(datos2$DIAGNOSIS), col = c("blue", "orange"), pch = c(4, 1), title = "Diagnosis")
# # text(1, 2, "A", col = "blue", pos = 4)
# # text(1, 1.8, "B", col = "orange", pos = 4)
```
We have the same problem, even bigger, if we try to do this visual analysis with groups of three variables.

### 4.2) Univariate and multivariate normality
Next we make a graphical exploration of the normality of the univariate distributions but considering now the diagnosis category. 
```{r}
datos3<-datos2[-1]
datos3

# Histogram representation of each variable for each diagnosis
par(mfcol = c(2, 3))
for (k in 2:31) {
  j0 <- names(datos3)[k]
  x0 <- seq(min(datos3[, k]), max(datos3[, k]), le = 50)
  for (i in 1:2) {
    i0 <- levels(datos3$DIAGNOSIS)[i]
    x <- datos3[datos3$DIAGNOSIS == i0, j0]
    hist(x, proba = T, col = grey(0.8), main = paste("Diagnosis", i0), xlab = j0)
    lines(x0, dnorm(x0, mean(x), sd(x)), col = "blue", lwd = 2)
  }
}

```

```{r}
# Representation of normal quantiles of each variable for each species
par(mfrow=c(2,3))
for (k in 2:31) {
  j0 <- names(datos3)[k]
  x0 <- seq(min(datos3[, k]), max(datos3[, k]), le = 50)
  for (i in 1:2) {
    i0 <- levels(datos3$DIAGNOSIS)[i]
    x <- datos3[datos3$DIAGNOSIS == i0, j0]
    qqnorm(x, main = paste("Diagnosis", i0, j0), pch = 19, col = i + 1)
    qqline(x)
  }
}
```
We could think that just a few of the variables in axis x and z follow a normal univariate distribution determined by their diagnosis. But then, for greater and numerical certainty, we do the respective normality tests.

```{r}
datos_tidy <- melt(datos3, value.name = "value")

results <- aggregate(formula = value ~ DIAGNOSIS + variable, data = datos_tidy, FUN = function(x){shapiro.test(x)$p.value})

results <- results[, c(2, 1, 3)]

# new column to find if the value obtained is greater than the p-value
results$NORMALITY <- ifelse(resultados$value < 0.05, "NO", "YES")

#Print results
print(results)
```

There's a high lack of univariate normality as we could imagine because of the previous results in section 2.5. There's a lack of multivariate normality aswell as we proved before in section 3.6. Therefore, the final phase of the analysis and linear model results are not well supported and also not reliable.

### 4.3) Homogeneity of variance

When multiple predictors are used, it must be verified that the covariance matrix is constant in all groups. In this case it is also advisable to check the homogeneity of the variance for each predictor at the individual level. So we apply now the most recommended test that is the Box M test, which is an extension of the Barttlet test for multivariate scenarios. 
It must be taken into account that it is very sensitive to whether the data are actually distributed according to a multivariate normal. For this reason, it is recommended to use a significance (p-value) <0.001 to reject the null hypothesis. However, the lack of MND let us think that this test won't provide useful information for the later model.

```{r}
boxM(data = datos3[, c(2:31)], grouping = datos3$DIAGNOSIS)
```
As we expected we reject the null hypothesis since p-value is practically zero and therefore we cannot assume homogeneity of variances. Also it is important to remember that for this conclusion to be reliable the assumption of multivariate normality must be met, which, in this case, that's not the case. We knew in advance that the results won't be positive and not even reliable.

### 4.4) Discriminat function

At this point our data do not satisfy the assumptions needed (multivariate normality and homogeneity of variance) so there wouldn't be recommendable continuing forward with the linear discriminant classification model. However, once again, we keep here the code to exemplify how it would be done.

```{r}
modelo_lda <- lda(formula = diagnosis ~ radius1 + texture1 + perimeter1 + area1 + smoothness1 + compactness1 + concavity1 + concave_points1 + symmetry1 + fractal_dimension1 +
                                         radius2 + texture2 + perimeter2 + area2 + smoothness2 + compactness2 + concavity2 + concave_points2 + symmetry2 + fractal_dimension2 +
                                         radius3 + texture3 + perimeter3 + area3 + smoothness3 + compactness3 + concavity3 + concave_points3 + symmetry3 + fractal_dimension3 ,
                  data = datos2)
modelo_lda
```
The output of this object shows us the prior probabilities of each group, in this case 0.6274165 for benign and 0.3725835 for malignant, the means of each regressor per group and the coefficients of the linear discriminant classification model.
Now we are able to classify new data based on its measurements by simply calling the predict function.

```{r}
#datos2
nuevas_observaciones <- data.frame(radius1 = 17.990, texture1 = 17.77, perimeter1 = 130.00, area1= 1203.0, smoothness1 = 0.09463, compactness1 = 0.23960, concavity1 = 0.1127000, concave_points1 = 0.080890, symmetry1 = 0.2087, fractal_dimension1 = 0.05742, radius2 = 0.5435, texture2 = 0.7813, perimeter2 = 3.3980, area2 = 24.320, smoothness2 = 0.006399, compactness2 = 0.024610, concavity2 = 0.0225400, concave_points2 = 0.014320, symmetry2 = 0.013890, fractal_dimension2 = 0.0045710, radius3 = 14.910, texture3 = 16.67, perimeter3 = 103.40	, area3 = 1606.0, smoothness3 = 0.16540, compactness3 = 0.54010, concavity3 = 1.105000, concave_points3 = 0.206000, symmetry3 = 0.4601, fractal_dimension3 = 0.07678)
predict(object = modelo_lda, newdata = nuevas_observaciones)
```
I pick only values from the first 10 rows, all classified as M (malignant) but the classifier output is class B (benign).

### 4.5) Model validation
We divide the original dataset into two subsets: the first, known as training set, with approximately 80% of the records, which will be used to fit a new model but always lineal one (model_lda_1 instead of model_lda); and the second, known as test set, with 20% of the remaining records, which will be used for cross-validation.
We use a new model to make the distinction between training and test datasets.

```{r}
set.seed(123)  # Set seed for reproducibility

# Number of total rows in the dataset
total_rows <- nrow(datos2)

# Number of rows for training (80%)
num_training <- round(0.8 * total_rows)

# Indices of rows for training (random)
indices_training <- sample(1:total_rows, num_training, replace = FALSE)

# Training set
training_set <- datos2[indices_training, ]

# Test set (using the remaining rows)
test_set <- datos2[-indices_training, ]

# Print information about the split
cat("Rows in the training set:", nrow(training_set), "\n")
cat("Rows in the test set:", nrow(test_set), "\n")
```

We visualize first the confusion matrix for the training set:

```{r}
# Dividing training vs. test set
diagnosis_training <- diagnosis[indices_training]
diagnosis_test <- diagnosis[-indices_training]

# Training LDA model
modelo_lda_1 <- lda(formula = diagnosis_training ~ radius1 + texture1 + perimeter1 + area1 + smoothness1 + compactness1 + concavity1 + concave_points1 + symmetry1 + fractal_dimension1 +
                                        radius2 + texture2 + perimeter2 + area2 + smoothness2 + compactness2 + concavity2 + concave_points2 + symmetry2 + fractal_dimension2 +
                                        radius3 + texture3 + perimeter3 + area3 + smoothness3 + compactness3 + concavity3 + concave_points3 + symmetry3 + fractal_dimension3 ,
                  data = training_set)

pred <- predict(modelo_lda_1, dimen = 1)
confusionmatrix(training_set$DIAGNOSIS, pred$class)
```
Confusion matrix for the training set.
```{r}
# Classification error percentage
trainig_error <- mean(training_set$DIAGNOSIS != pred$class) * 100
paste("trainig_error=", trainig_error, "%")
```
We visualize now the training set classification error percentage but we are interested in the test set error because it is where the validation is usually done. 
```{r}
# Predictions on the test set
pred_test <- predict(modelo_lda_1, newdata = test_set, dimen = 1)

# Confusion matrix for the test set
confusion_matrix_test <- table(test_set$DIAGNOSIS, pred_test$class)
print(confusion_matrix_test)
```
Confusion matrix for the test set.
```{r}
# Classification error percentage for the test set
test_error <- mean(test_set$DIAGNOSIS != pred_test$class) * 100
paste("test_error =", test_error, "%")
```
In this case the correct classifications rate is 95.61% aprox. It seems like a good result but in terms of cancer diagnosis maybe it is not enough (We are not medical experts to judge that).

### 4.6) Displaying rankings
We try to display an example considering all the dataset
```{r}
partimat(DIAGNOSIS ~  radius1 + texture1 + perimeter1 + area1 + smoothness1 + compactness1 + concavity1 + concave_points1 + symmetry1 + fractal_dimension1 ,
         data = datos2[-1], method = "lda", prec = 200,
         image.colors = c("green", "orange"),
         col.mean = "yellow",nplots.vert =1, nplots.hor=3)
```
We could see how chaotic the results are if we make with all the dataset instead of only the test-set. 
Now we display some examples with the first axis variables and the test set without the intention of obtaining good results.
```{r}
partimat(diagnosis_test ~  radius1 + texture1 + perimeter1 + area1 + smoothness1 + compactness1 + concavity1 + concave_points1 + symmetry1 + fractal_dimension1 ,
         data = test_set, method = "lda", prec = 200,
         image.colors = c("green", "orange"),
         col.mean = "yellow",nplots.vert =1, nplots.hor=3)
```
In case of using the test set, we could visualize better the points.
Results: Every classification has at least an error of 10% more or less, few of then even up to 30% or very close to it. So they're not good models. 

Let's try with radius1, area1, concave_points1, radius3, area3, perimeter3 and concave_points3. Variables that seemed before they were good individual options to build the classifier.
Let see how "they work in pairs" cosidering again all the dataset.
```{r}
partimat(DIAGNOSIS ~ radius1 + area1 + concave_points1 + radius3 + area3 + perimeter3 + concave_points3 ,
         data = datos2[-1], method = "lda", prec = 200,
         image.colors = c("green", "orange"),
         col.mean = "yellow",nplots.vert =1, nplots.hor=3)
```
The classifiers built with 3 type variables (axis z variables) have a bit lower error, specially the last three classifiers (concave_points3 with radius3, area3 and perimeter3). They have an error equal to 6%. The best pairs-classifiers obtained until now. So if a classification model were made with two predictors, this pair would be considered.

## **5. Quadratic Discriminant Analysis (QDA)**

As for the Linear Discriminant Analysis, to carry out the Quadratic Discriminant Analysis we should begin with the graphical exploration of the data and the checks about the hypothesis on univariate and multivariate normality and homogeneity of the variances, which have already been previously done. Then we now that there's not assumption of MNV, nevertheless the quadratic discriminant analysis has a certain robustness in this case. Also there's NO homogeneity of variances.


### 5.1 Discriminant Function
Although the assumption of multivariate normality is not verified, taking into account that the variances are not homogeneous, a quadratic discriminant model is adjusted because it is robust against the lack of this assumption, although it must be kept in mind given the possibility of obtaining unexpected results.

```{r}
#quadratic model
modelo_qda <- qda(diagnosis_training ~ radius1 + texture1 + perimeter1 + area1 + smoothness1 + compactness1 + concavity1 + concave_points1 + symmetry1 + fractal_dimension1 +
                               radius2 + texture2 + perimeter2 + area2 + smoothness2 + compactness2 + concavity2 + concave_points2 + symmetry2 + fractal_dimension2 +
                               radius3 + texture3 + perimeter3 + area3 + smoothness3 + compactness3 + concavity3 + concave_points3 + symmetry3 + fractal_dimension3, 
                  data = training_set)
modelo_qda
```
The output of this object shows us the prior probabilities of each group. Now they are a bit different from those ones obtained in model_lda because in this case we are using just the training set data instead of all the dataset. It also shows the means of each regressor per group.

Once the classifier is built, we can classify new data based on its measurements by simply calling the predict function. We classify all the observations in the test set.
```{r}
nuevas_observaciones <- test_set
predict(object = modelo_qda, newdata = nuevas_observaciones)
```

### 5.2 Model validation
```{r}
# Confusion matrix
pred <- predict(object = modelo_qda, newdata = test_set)
confusionmatrix(test_set$DIAGNOSIS, pred$class)
```

```{r}
# Classification error percentage
test_error <- mean(test_set$DIAGNOSIS != pred$class) * 100
paste("test_error=", test_error, "%")
```
In this case the correct classifications rate is 93.86%.

### 5.3 Displaying rankings
Analogous to LDA part, we display some graphics to check quadratic classifiers grouping variables by pairs. Always considering tghe testset data.

Example with axis x variables:
```{r}
partimat(formula = diagnosis_test ~ radius1 + texture1 + perimeter1 + area1 + smoothness1 + compactness1 + concavity1 + concave_points1 + symmetry1 + fractal_dimension1,
                                    #radius2 + texture2 + perimeter2 + area2 + smoothness2 + compactness2 + concavity2 + concave_points2 + symmetry2 + fractal_dimension2 +
                                    #radius3 + texture3 + perimeter3 + area3 + smoothness3 + compactness3 + concavity3 + concave_points3 + symmetry3 + fractal_dimension3,
         data = test_set, method = "qda", prec = 400,
         image.colors = c("skyblue2", "darkgoldenrod1"),
         col.mean = "green", nplots.vert =1, nplots.hor=3)
```
Results with best individual variables considered:
```{r}
partimat(formula = diagnosis_test ~ radius1 + area1 + concave_points1 + radius3 + area3 + perimeter3 + concave_points3,
         data = test_set, method = "qda", prec = 400,
         image.colors = c("skyblue2", "darkgoldenrod1"),
         col.mean = "green", nplots.vert =1, nplots.hor=3)
```

We try now considering all the dataset:
```{r}
partimat(formula = DIAGNOSIS ~ radius1 + area1 + concave_points1 + radius3 + area3 + perimeter3 + concave_points3,
         data = datos2[-1], method = "qda", prec = 400,
         image.colors = c("skyblue2", "darkgoldenrod1"),
         col.mean = "green", nplots.vert =1, nplots.hor=3)
```
Similar to LDA case, the best classification of the data is the one given by the predictors radius3 and concave_points3 but now with more accuracy (particular and general case).
Then we could confirm that with a large number of training observations available, QDA is more appropriate than LDA (also in our case we couldn't rely on LDA results).


## **6. Cluster Analysis**
Additionally, we are performing a cluster analysis to confirm that the grouping of the response variable used in the classification models (DIAGNOSIS) is appropriate.

### 6.1 Data Preparation
To perform a cluster analysis with R, the data must be prepared as follows:
  1 • Ensure that the rows are records of observations and that the columns are the variables of interest (data.frame type structure).
  2 • The missing values must be adequately treated (eliminate or replace their value).
  3 • It must be decided whether to work with standardized data to make variables measured on different scales comparable.
  
In our case: datos2[-2] verifies 1), there's no missing values in our dataset so we also verifies 2)

```{r}
# Print our dataset without the 2 first columns which are not of interest in cluster analysis (in fact, column 2 is what we want to determine with this CA and column 1 is for IDs).
datos_cluster <- datos2[, -c(1, 2)]
head(datos_cluster)
```

We check means and standard deviations to establish if the data are standardized:
```{r}
columnas <- colnames(datos_cluster)
medias <- list()
sd <- list()

for (col in columnas){
  medias <- c(medias, mean(datos_cluster[[col]], na.rm = TRUE))
  sd <- c(sd, sd(datos_cluster[[col]], na.rm = TRUE))
}
print(paste("Medias:", paste(medias, collapse = ", ")))
```

```{r}
print(paste("Desviaciones estándar:", paste(sd, collapse = ", ")))

```

Our data are not standardized because they doo't have a mean of 0 and a standard deviation of 1, so 3) is not verified and we correct it:
```{r}
# To prevent the cluster analysis from being influenced by any arbitrary variable, the data are standard
datos_cluster<-as.data.frame(scale(datos_cluster))
# Visualization of standardized data
head(datos_cluster)
```

Now our data are prepared for CA.

### 6.2 Distance Information
To classify observations into groups it is necessary to choose appropriate measures of similarity, or distance (dissimilarity), that provide information on how similar any two observations are. Almost all the usual software for cluster analysis uses the Euclidean distance so we opt for using this one as R default option.

The following distance matrix shows in brown those states that present large dissimilarities (distances), compared to those that seem closer in yellow. The color white is used to refer to those states with distances that are not so extreme as to be considered low or high.
```{r}
distance<- get_dist(datos_cluster)
fviz_dist(distance, gradient = list(low ="yellow", mid = "white", high = "brown"))
```
### 6.3 Hierarchical clustering: Ward’s method
Hierarchical clustering is interested in finding a hierarchy based on the closeness or similarity of the data according to the distance considered. In the agglomerative case, we start from a group with the closest observations. The next closest pairs are then calculated and groups are generated in an ascending manner.
This construction can be observed visually by means of a dendrogram and the selection of the optimal number of groups can be also estimated from this same graph.
```{r}
dendrogram <- hclust(dist(datos_cluster, method = 'euclidean'), method = 'ward.D')
ggdendrogram(dendrogram, rotate = FALSE, labels = FALSE, theme_dendro = TRUE) +
labs(title = "Dendrograma")
```
On the horizontal axis of the dendrogram we have each of the data that make up the input set, while on the vertical axis the Euclidean distance that exists between each group is represented, as that these are becoming hierarchical.
Each vertical line in the diagram represents a grouping. (UP -> less groups / DOWN -> more groups)

One way to determine the appropriate K number of groups is to cut the dendrogram at that height of the diagram that best represents the input data. However, we are going to explore other options.

### 6.4 Determination of the optimal number of clusters

Below are three of the most used methods to determine this optimal number of groups: Elbow method, Silhouette method and the Gap statistic. Let's see if they match.

```{r}
#Elbow method
set.seed(123)
fviz_nbclust(datos_cluster, kmeans, method = "wss")
```
We could appreciate the "elbow" for the number of 2 clusters.
```{r}
#Silhouette Method
set.seed(123)
fviz_nbclust(datos_cluster, kmeans, method = "silhouette")
```

```{r}
#Gap Statistical Method
set.seed(123)
gap_stat <- clusGap(datos_cluster, FUN = kmeans, nstart = 25,K.max = 10, B = 50)
fviz_gap_stat(gap_stat)
```
### 6.5 Analysis of results
After the detailed analysis of the optimal number of clusters, carried out in the previous section, K=2 seems to be the most appropriate number of groups for this analysis.
It matches with our original dataset which classifies data into 2 groups, M and B.

Finally, when we know the optimal number of clusters, we use non-hierarchical clustering through the K-means algorithm to classify data into this 2 groups and also get more cluster analysis information. 
We use then K-means function.
```{r}
set.seed(123)
k2 <- kmeans(datos_cluster, 2, nstart = 25)

# Displaying all the fields of the object k2
str(k2)
```

A visual way to summarize the results elegantly and with straightforward interpretation is by using the fviz_cluster function.
```{r}
set.seed(123)
fviz_cluster(k2,data=datos_cluster)
```
The last thing, like an interesting point, is that it is possible to extract the clusters and add them to our initial data to provide some cluster-level descriptive statistics. We already have this information in our original dataset but we show how it works.
```{r}
datos_cluster %>%
mutate(Cluster = k2$cluster) %>%
group_by(Cluster) %>%
summarise_all("mean")
```