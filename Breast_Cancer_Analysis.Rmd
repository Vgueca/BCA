---
output:
  pdf_document: default
  html_document: default
---

# BREAST CANCER ANALYSIS

Along this Rnotebook we will analyze the features obtained from the [Breast Cancer's Dataset](https://archive.ics.uci.edu/dataset/17/breast+cancer+wisconsin+diagnostic). We will apply the methods and techniques seen during the course of Multivariate Statistics of the University of Granada.

## **1. Loading packages and data sets**

### 1.1 Loading and installing R packages

```{r warning=FALSE, message=FALSE}

#########################################
# Loading necessary packages and reason #
#########################################

# This is an example of the first installation of a package
# Only runs once if the package is not installed
# Once it is installed this sentence has to be commented (not to run again)
#install.packages("ggplot2")
#install.packages("summarytools")
#install.packages("tidyverse")
#install.packages("pastecs")

# Package required to call 'freq' and 'descr' functions (descriptive statistics)
library(summarytools)

# Package required to call 'ggplot' function
library(ggplot2)

# Package required to call 'ggarrange' function
library(ggpubr)

# Package required to call 'scatterplot3d' function
library(scatterplot3d)

# Package required to call 'melt' function
library(reshape2)

# Package required to call 'mvn' function
library(MVN)

# Package required to call 'boxM' function
library(biotools)

# Package required to call 'summarise' function
library(dplyr)

# Package required to call 'createDataPartition' function
library(caret)

# Package required to call 'read.spss' function (loading '.spss' data format)
library(foreign)

# Package required to call 'read_xlsx' function (loading '.xlsx' data format)
library(readxl)

# Package required to load the data set 'RBGlass1'
library(archdata)

# Package required to call 'cortest.bartlett' function
library(psych)

# Package required to call 'fviz_pca_var, fviz_pca_ind and fviz_pca' functions
library(factoextra)

# Package required to call 'scatterplot3d' function
library(scatterplot3d)

# Package required to call 'factanal' function
library(stats)

# Package required to call 'freq' function
library(summarytools)

# Package required to call 'cortest.bartlett' function
library(psych)

# Package required to call 'hetcor' function
library(polycor)

# Package required to call 'ggcorrplot' function
library(ggcorrplot)

# Package required to call 'corrplot' function
library(corrplot)

# Package required to call 'rplot' function
library(corrr)

#  Package required to call 'ggplot' function
library(ggplot2)
library(ggpubr)


library(tidyverse)

```

### 1.2 Loading data set

```{r}

#file path

#Valentin's path
#setwd("C:/Users/valentin/Desktop/DGIIM 5/1 CUATRI/MULTIVARIANTE/PRACTICAS/PRACTICA_FINAL/Breast_Cancer_Analysis")

#Javi's path
setwd("/home/javi5454/Desktop/Github/Breast_Cancer_Analysis")

#loading data. We haven seen that the file has the values of each feature separated by "," so we use as argument the specific separator ",". The file does not have header so we also specify it.
dataframe <- read.table("wdbc.data", header = FALSE, sep=",")

#verifying data.
head(dataframe)
```

In the previous table we can see that the features has not the right name, even, in the table there are some feature that we are not interesting in our study, as they are used in machine learning's context to make predictions and not in multivariate analysis.

In the following cells we will preprocess the table to erase these columns, and to rename the features correctly, before starting with any research of our data.

```{r}

#Columns' names from the dataset description
colnames<-c('ID','DIAGNOSIS','radius1', 'texture1','perimeter1','area1','smoothness1','compactness1','concavity1','concave_points1','symmetry1','fractal_dimension1','radius2','texture2','perimeter2','area2','smoothness2','compactness2','concavity2','concave_points2','symmetry2','fractal_dimension2','radius3','texture3','perimeter3','area3','smoothness3','compactness3','concavity3','concave_points3','symmetry3','fractal_dimension3')

#changing columns' names
colnames(dataframe)<- colnames

#erasing the first two columns 
datos <- dataframe[, -c(1, 2)]

#finally we add an extra column to our dataframe which funtion is just identify each row, but regarding our order and not the dataframe's one.
datos$ID <- 1:nrow(datos)
datos <- datos[, c('ID', names(datos)[-ncol(datos)])]

#showing the data to verify the changes
datos
```

```{r}
#discomment in case the file with the preprocessed dataframe is needed.
#write.table(datos, file = 'dataframe.csv', sep = ',', row.names = FALSE)
```

### 1.3 Data Base description

The file *wdbc.data* contains data collected from 569 patients based on 10 different indicators of breast cancer. We have 3 different measures for each indicator, each one corresponding to a spacial dimensions (XYZ axis):

-   Radius (mean of distances from center to points on the perimeter).

-   Texture (standard deviation of gray-scale values).

-   Perimeter.

-   Area.

-   Smoothness (local variation in radius lengths).

-   Compactness ($\frac{perimeter^2}{area - 1.0}$).

-   Concavity (severity of concave portions on the contour).

-   Concave points (number of concave portions of the contour).

-   Symmetry.

-   Fractal dimension ("coastline approximation" - 1).

## 2. Univariate Exploratory Analysis

Firstly, we would like to give an brief description of the data, to be more aware of what types of data we are working on.

The public dataset we are working on is related to the Breast Cancer. All the values given are computed from digitized images of a fine needle aspirate (FNA) of a breast mass. They describe **characteristics of the cell nucleus**.

The dataframe consists on 30 continuous and numeric features (plus the ID and Target one). Each feature is actually a real-value computed for each cell nucleus.

In our opinion, we belief that this dataset is clearly a dataset to perform Machine Learning, as they even included one Target feature. The features are intuitive and the idea behind the dataset is also clear and concise; predict the type of Breast Cancer (malignant or bening).

So, after this quick analysis of the dataset we can start to perform the Univariate Exploratory Analysis.

### 2.1 Data grouping or recoding

Regarding the types of values we have on the dataset and the range of the domain where the features take their values, we are not interested on perform data-grouping or to recode them.

### 2.2 Missing values

Another key part of the Univariate Exploratory Analysis is to identify missing values. On the following cells we are going to compute the porcentage of missing values of each feature. ![]()

```{r}
# compute the missing values for each feature of our dataset
missing_values <- sapply(datos, function(x) sum(is.na(x))/length(x) * 100)

# we plot the results of the missing values data 
barplot(missing_values, main = "Porcentage of missing values", xlab = "", ylab = "Porcentage of missing values", col = "steelblue", las = 2, ylim = c(0, 100))

```

In the plot we can see clearly that **there are not missing values.** We are not need in our case to study the random pattern of the missing values.

If we would have obtained more than 5% of missing values on any feature, we would have to analyze the random patterns. As we have continuous variables we would have needed to apply a Student test. Regarding the results of the applied test we would have obtained that the pattern is random or not, and in both cases, replacing the missing values by the mean or median.

### 2.3 Classical numeric descriptive analysis

Following, we are going to compute basic statistics measures to gather more information about or data. As the variables are quantitative, we are going to compute **basic numerical descriptive statistics, their histograms, density and boxplot.**

```{r}
# for every feature except the ID
for(col in names(datos)[-1]) {

  cat("\nSummary of the feature: ", col, "\n")
  
  # showing the summary
  print(summary(datos[[col]]))
}

```

As the function we have used to display this values is maybe too simple, we are going to display more measures about our features to understand better our dataset.

```{r}
#compute the measures for all features except the ID
breast_cancer_dataset <- datos[, -1]
descr_res <- descr(breast_cancer_dataset)

#show adequately the table
print((descr_res), method = 'render')
```

Next, we will plot some interesting graphs of each feature, such as density function, histogram and boxplot.

```{r}

for(col in names(datos)[-1]) {

  # creating density, histogram and boxplot
  p1 <- ggplot(datos[-1], aes_string(x = col)) + geom_density() +
    labs(title = paste("Density function of", col), x = col, y = "Values") +
    theme(plot.title = element_text(size = 8, hjust = 0.5))
  
  p2 <- ggplot(datos[-1], aes_string(x = col)) + geom_histogram(bins = 30) +
    labs(title = paste("Histogram of", col), x = col, y = "Values") +
    theme(plot.title = element_text(size = 8, hjust = 0.5))
  
  p3 <- ggplot(datos[-1], aes_string(x = col)) +
    geom_boxplot(outlier.colour = "red", outlier.shape = 1, outlier.size = 2) +
    coord_flip() + labs(title = paste("Boxplot of", col), x = "Values", y = "") +
    theme(plot.title = element_text(size = 8, hjust = 0.5))
  
  # show the graphs
  print(ggarrange(p1, p2, p3, nrow = 1, common.legend = FALSE))
}


```

Just as an addition to all information gathered, we are interested on seeing the Target feature of the dataset, "Diagnosis". Therefore, we will show some graphical outputs about this categorical variable.

```{r}
# frequency tables
freq(dataframe[2])
```

Just as a reminder, B = benign and M = malignant.

```{r}

# create the pie chart and bar graph for the Target column (DIAGNOSIS)
p1 <- ggplot(dataframe, aes(x = factor(1), fill = DIAGNOSIS)) + geom_bar() +
  coord_polar("y") + labs(x = "DIAGNOSIS", y = "%")
  
p2 <- ggplot(dataframe, aes(x = factor(1), fill = DIAGNOSIS)) + geom_bar() +
  labs(x = "DIAGNOSIS", y = "%")

# display the graphs
print(ggarrange(p1, p2, nrow = 1, ncol = 2, common.legend = TRUE))
```

### 2.4 Outliers

In this section we will try to identify outliers in each feature as some methods we will use along this proyect are sensitive to the presence of outliers.

We have to remind that all our features are numerical variables so we can apply some graphs methods to identify intuitively the outliers. It is remarkable that firstly we standarize the data because, as we can see in the previous sections, our features have different scales.

```{r}

# standarize
sca <- scale(datos[-1])

# transform data to large format
datos_largos <- as.data.frame(sca) %>% 
  rownames_to_column(var = "ID") %>% 
  pivot_longer(cols = -ID, names_to = "Variable", values_to = "Valor")

# reformat the graphical options
options(repr.plot.width = 10, repr.plot.height = 8)

# create boxplot
p <- ggplot(datos_largos, aes(x = Variable, y = Valor, fill = Variable)) +
  geom_boxplot() +
  theme(axis.text.x = element_text(angle = 90, hjust = 1)) +
  labs(title = "Outliers", x = "All explanatory variables", y = "Values") +
  scale_fill_discrete(name = "Variables") 

# display the graphs
print(p)

```

Another way of outliers' visualization:

```{r}

columnas <- colnames(datos[-1])

# boxplot for each feature
for (col in columnas) {
  p <- ggplot(datos[-1], aes_string(x = col)) +
    geom_boxplot(outlier.colour = "red", outlier.shape = 1, outlier.size = 2) +
    coord_flip() +
    labs(title = paste("Boxplot of", col), x = "Values", y = "")
  
  # show the graph
  print(p)
}

```

In both graphs we can see that there are outliers in almost every feature. Regarding this, we can eliminate them or replace by the mean or median. As this is not rigurous we are going to take the one option who give us better results.

```{r}
# ---   OUTLIERS METHODOLOGY ---

# replacing outliers by the mean
reemplazar_outliers <- function(x) {
  qnt <- quantile(x, probs=c(.25, .75), na.rm = TRUE)
  H <- 1.5 * IQR(x, na.rm = TRUE)
  outliers <- x < (qnt[1] - H) | x > (qnt[2] + H)
  x[outliers] <- mean(x[!outliers])
  return(x)
}



# we apply the function to every feature of our dataset
datos_preprocessed <- datos %>% mutate(across(-1, reemplazar_outliers))


```

```{r}
# Uncomment to undo the outliers treatment
# datos_preprocessed = datos
```

We have created a function that erase the outliers, just taking in count the quantiles of each feature and leaving the values which are not considered as outliers.

We will replot the boxplot of each feature with the changes.

```{r}
columnas <- colnames(datos_preprocessed[-1])

# boxplot for each feature
for (col in columnas) {
  p <- ggplot(datos_preprocessed[-1], aes_string(x = col)) +
    geom_boxplot(outlier.colour = "red", outlier.shape = 1, outlier.size = 2) +
    coord_flip() +
    labs(title = paste("Boxplot of", col," (without outliers)"), x = "Values", y = "")
  
  # show the graph
  print(p)
}
```

### 2.5 Univariate normality

As many techniques can not avoid the assumption of normality is necessary to check the distribution of each feature of the dataset. In order to analyze their distribution we are going to use two techniques. The first one is graphical, using the funcion qqplot, and the second one is about applying the Univariate normality test (Shapiro-Wilk).

```{r}
columnas <- colnames(datos_preprocessed[-1])

# creating qqplot
for (col in columnas) {
  
  p <- ggplot(data = datos_preprocessed, aes(sample = get(col))) +
    stat_qq(distribution = qnorm, dparams = list(mean = mean(datos_preprocessed[[col]], na.rm = TRUE), sd = sd(datos_preprocessed[[col]], na.rm = TRUE))) +
    geom_abline(color = "red", linetype = "dashed") +  # Agrega una línea diagonal
    ggtitle(paste("Q-Q plot of", col)) +
    xlab("Theoretical Quantiles") +
    ylab("Sample Quantiles")
  
  print(p)
}


```

```{r}

columnas <- names(datos_preprocessed)[-1]

# Crea un histograma para cada característica
for (j0 in columnas) {
  # Genera el histograma
  p <- ggplot(datos_preprocessed, aes_string(x = j0)) +
    geom_histogram(aes(y = ..density..), bins = 30, fill = "lightblue", color = "black") +
    geom_density(color = "red", lwd = 1) +
    labs(title = paste("Histogram of", j0), x = j0, y = "Density")
  
  # Muestra el histograma
  print(p)
}

```

With this exploratory analysis we can have a general idea of the possible normal distribution of the univariate variables. However, as these are graphical outputs and the test is just our "sight" we will apply some more mathematical and numerical test to accept the assumption of normality.

The null hypothesis of the test is that the data **follow a univariate normal distribution.** This hypothesis is rejected if the p-value given by the Shapiro-Wilk test is less than 0.05.

```{r}
#reformat the data
datos_tidy <- melt(datos_preprocessed, id.vars = "ID", variable.name = "variable", value.name = "value")

# Shapiro-Wilk test for each variable.
resultados <- aggregate(value ~ variable, data = datos_tidy, FUN = function(x){shapiro.test(x)$p.value})

# new column to find if the value obtained is greater than the p-value
resultados$NORMALITY <- ifelse(resultados$value < 0.05, "NO", "YES")

# display results
print(resultados)

```

Looking on the results obtained we can see that despite we obtained in the graphical outputs that the most part of the variables followed a normal distribution, actually they do not follow a normal distribution. Thanks to the Shapiro-Wilk test we were able to prove it, and it is a important test always to contrast what "we think" with the mathematically right results.

## 3. Multivariate Exploratory Analysis

### 3.1 Correlation

Firstly, it is necessary to check if the variables are or not independent. We could do it observing the correlation matrix, at sample level. At population level, we could check if there is correlation using Bartlett's Test (this test is used to test the null hypothesis, $H_0$ that all $k$ population variances are equal against the alternative that at least two are different).

```{r}
###############################
# Correlation at sample level #
###############################

# Are the variables correlated at sample level?
correlation_matrix<-cor(datos_preprocessed[-1])
correlation_matrix
```

```{r}
det(correlation_matrix)
```

It is noticed an important correlation between some variables. For example, we can see it between perimeter1 and area:

```{r}
cor(datos_preprocessed$perimeter1, datos_preprocessed$area1)
```

Let's study correlation at population level

```{r}
###################################
# Correlation at population level #
###################################

# Bartlett's sphericity test:
# This test checks wheter the correlations are significantly different from 0
# The null hypothesis is H_0; det(R)=1 means the variables are uncorrelated
# R denotes the correlation matrix
# cortest.bartlett function in the package pysch performs this test
# This function works with standarized data

#Standardization
datos_preprocessed_scale<-scale(datos_preprocessed[-1])

# Bartlett's sphericity test
cortest.bartlett(cor(datos_preprocessed_scale), 569) # 569 is the sample size
```

Due to $p-value$ is 0\<0.001, we can assume that there is no independence, so it is logical to propose dimension's reduction trough a Principal Component Analysis or Factorial Analysis.

### 3.2 Principal Components Analysis (PCA)

The following code performs the PCA, obtaining the eigenvectors of each component and them eigenvalues (variance of each one).

```{r}
# The 'prcomp' function in the base R package performs this analysis
# Parameters 'scale' and 'center' are set to TRUE to consider standardized data
PCA<-prcomp(datos_preprocessed[-1], scale=T, center=T)

# The field 'rotation0 of the 'PCA' object is a matrix
# Its columns are the coefficients of the principal components
# Indicates the weight of each variable in the corresponding principal component
PCA$rotation
```

At field "sdev" PCA object we can find standard deviations of each principal component. With summary function applied to PCA object, we obtain some interesting measures:

```{r}
PCA$sdev
```

```{r}
summary(PCA)
```

The following graph shows the proportion of explained variance

```{r}
explained_variance<-PCA$sdev^2 / sum(PCA$sdev^2)

p1<-ggplot(data=data.frame(explained_variance, pc=1:ncol(datos_preprocessed[-1])), 
           aes(x= pc, y = explained_variance, fill=explained_variance)) + geom_col(width = 0.3) +
           scale_y_continuous(limits = c(0,0.6)) + theme_bw() + labs(x = "Principal component", y = "Proportion of variance")

p1
```

The following graph shows the proportion of cumulative explained variance

```{r}
cummulative_variance<-cumsum(explained_variance)

p2<-ggplot(data = data.frame(cummulative_variance, pc= 1:ncol(datos_preprocessed[-1])), aes(x = pc, y = cummulative_variance, fill=cummulative_variance)) + geom_col(width = 0.5) + scale_y_continuous(limits = c(0,1)) + theme_bw() + labs(x = "Principal component", y = "Proportion of cumulative variance")

p2
```

### 3.3 Appropriate number of principal components

There are different methods to carry out this process, but we will use Rule of Abdi et al. (2010): The variances explained by the principal components are averaged and those whose proportion of explained variance exceeds the mean are selected.

```{r}
PCA$sdev^2
```

```{r}
mean(PCA$sdev^2)
```

```{r}
counter<-1
print("Prinicipal components considered:")
for(element in PCA$sdev^2){
  if(element > mean(PCA$sdev^2)){
    print(colnames(PCA$x)[counter])
  }
  counter<-counter + 1
}
```

So, we would only consider the first 6th components. They accumulate near to 82% of the explained variance. Each principal component is obtained as linear combination of all components with the coefficients indicated at the columns of rotation matrix.

There is the possibility of representing different pairwise comparisons between the principal components, but since we have 6 principal components, we would have to create $\frac{6!}{2} = 360$ graphs, which is impractical. Instead, we will only make the comparison graphs between first and second principal component, which are in fact, the two ones with most part of the explicable variance.

Firstly, the following graphical outputs show the projection of the variables in two dimensions. Display the weight of the variable in the direction of the principal component.

```{r}
p1<-fviz_pca_var(PCA,repel = T, col.var = "cos2", legend.title="Distance", title="Variables")+theme_bw()

p1
```

It is also possible to represent the observations. As well as identify with colors those observations that explain the greatest variance of the principal components.

```{r}
p1<-fviz_pca_ind(PCA, col.ind = "contrib", gradient.cols = c("#00AFBB", "#E7B800", "#FC4E07"), 
                 repel = T, legend.title = "Contrib.var", title ="Recrods") + theme_bw()

p1
```

Now, joint representation of variables and observations. Relates the possible relationships between the contributions of the records to the variances of the components and the weight of the variables in each principal component.

```{r}
p1<-fviz_pca(PCA, alpha.ind = "contrib", col.var="cos2", col.ind="seagreen", gradient.cols = c("#FDF50E", "#FD960E", "#FD1E0E"), repel = T, legend.title = "Distancia") + theme_bw()

p1
```

### 3.4 Factorial Analysis

First of all in this section, different graphical outputs are illustrated below that provide an intuitive idea of correlation between the variables. A *trained eye* could anticipate the appropriate number of factor with this visual information.

```{r}
# Polychoric correlation matrix
poly_cor<-hetcor(datos_preprocessed[-1])$correlations

ggcorrplot(poly_cor, type="lower", hc.order = T)
```

Another interesting visual representation is the following:

```{r}
corrplot(cor(datos_preprocessed[-1]), order="hclust", tl.col="black", tl.cex = 1)
```

One last representation is the following:

```{r}
datos_preprocessed_correlations<-correlate(datos_preprocessed[-1]) #Correlations object
rplot(datos_preprocessed_correlations, legend = T, colours = c("firebrick1", "black", "darkcyan"), print_cor = T)
```

Observing the graphical outputs, it seems to be between 3 and 4 groups for latent variables with high correlation with a group of observable variables and low correlation with the others. It can help us to decide which is the optimum number of factor for our Factorial Analysis.

Now, we must choose a method to extract the factors: main factor, likelihood, etc. First, we will compare the outputs of the maximum likelihood method and minimal residual model. We will calculate 3 factors due to the last graphs.

```{r}
### Test of two models with three factors
model1<-fa(poly_cor, nfactors = 3, rotate = "none", fm="mle") #likhelihood method

model2<-fa(poly_cor, nfactors = 3, rotate = "none", fm = "minres") #minimal residual model

# Outputs of these models: factorial matrices, etc.

print("Model 1: maximum likelihood esthimator")
model1$loadings

print("Model 2: minimal residual model")
model2$loadings
```

Now, a comparison of the communalities of these two methods is illustrated. It appears that communalities of the likelihood model are lower than those of the minimum residual model.

```{r}
# Comparing communalities
sort(model1$communality, decreasing = T)->c1
sort(model2$communality, decreasing = T)->c2

cbind(c1,c2)
```

A comparison of uniqueness is also performed. That is the proportion of variance that has not been explained by the factor (1 - communality)

```{r}
sort(model1$uniquenesses, decreasing = T)->u1
sort(model2$uniquenesses, decreasing = T)->u2
cbind(u1,u2)
```

We can observe that with the minimal residual model we obtain higher values, then latent factors obtained with these methods explains better the variance of observed variables.

### 3.5 Choosing the optimal number of factors to consider

There are different criteria, among which the Scree plot and parallel analysis stand out. The **Scree plot** consist in represent the scree plot obtained from the representation of eigenvalues in descendant order at Y-axis and the number of latent factors at X-axis. Joining the points we obtain a figure that starts with a strong drop and then a leveling-off. The methods indicates that we have to choose the number of factors where the graphical output makes an "elbow". The parallel analysis is a technique designed to help take some of the subjectivity out of interpreting the scree plot. Graphically, it identifies the point where the cumulative explained variance of real data significantly surpasses the average cumulative explained variance of random data, determining the optimal number of factors to retain. This method helps prevent overextraction of factors.

```{r}
# Scree plot
scree(poly_cor)
```

```{r}
#Parallel anaylisis
fa.parallel(poly_cor, n.obs=100, fa="fa", fm="minres")
```

In this case, we will chose 5 factors due to parallel analysis.

Other way to study the optimal number of factors is with the following hypothesis test, which contrast if the number of factors is enough or not.

```{r}
factanal(datos, factors=5, rotation="none", lower=0.02)
```

So, we do not reject the null hypothesis, and we will use 5 factors.

Finally, we will make an estimation of the factorial model with 5 factors. We implement a varimax rotation to seek a simpler interpretation.

```{r}
varimax_model<-fa(poly_cor, nfactors=5, rotate="varimax", fa="minres")
```

We obtain a warning telling us an ultra-Heywood case was detected. Since communalities are squared correlations, you would expect them always to lie between 0 and 1. It is a mathematical peculiarity of the common factor model, however, that final communality estimates might exceed 1. If a communality exceeds 1, it is an ultra-Heywood case. It implies that some unique factor has negative variance, a clear indication that something is wrong. One possible cause is that we are using too much factors. Let's try with 3 factors:

```{r}
varimax_model<-fa(poly_cor, nfactors=3, rotate="varimax", fa="minres")
```

Here we do not get any warning of an ultra-Heywood case. We will run again our previous hypothesis test, but now with three factors.

```{r}
factanal(datos, factors=3, rotation="none", lower=0.03)
```

So, then we will use 3 factors in our model. Now, the rotated factorial matrix is shown.

```{r}
print(varimax_model$loadings, cut = 0)
```

Visually we could make the effort to see what variables each correlates with one of the factors, but it is very tedious. So we use the following representation in diagram mode.

```{r}
fa.diagram(varimax_model)
```

### 3.6 Multivariate Normality of the data

As the multivariate normality can be affected by the presence of multivariate outliers we will plot again the outliers in relation with the Chi-Square qqplot.

```{r}
outliers <- mvn(data = datos[,-1], mvnTest = "hz", multivariateOutlierMethod = "quan")
```

Therefore, as the number of outliers is enough small, we can perfom the following tests in order to find some evidence of lack of multivariate normality.

```{r}
royston_test <- mvn(data = datos_preprocessed[,-1], mvnTest = "royston", multivariatePlot = "qq")
royston_test$multivariateNormality

```

```{r}
hz_test <- mvn(data = datos_preprocessed[,-1], mvnTest = "hz")
hz_test$multivariateNormality
```
