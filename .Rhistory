# showing the summary
print(summary(datos[[col]]))
}
#compute the measures for all features except the ID
breast_cancer_dataset <- datos[, -1]
descr_res <- descr(breast_cancer_dataset)
#show adequately the table
print((descr_res), method = 'render')
for(col in names(datos)[-1]) {
# creating density, histogram and boxplot
p1 <- ggplot(datos[-1], aes_string(x = col)) + geom_density() +
labs(title = paste("Density function of", col), x = col, y = "Values") +
theme(plot.title = element_text(size = 8, hjust = 0.5))
p2 <- ggplot(datos[-1], aes_string(x = col)) + geom_histogram(bins = 30) +
labs(title = paste("Histogram of", col), x = col, y = "Values") +
theme(plot.title = element_text(size = 8, hjust = 0.5))
p3 <- ggplot(datos[-1], aes_string(x = col)) +
geom_boxplot(outlier.colour = "red", outlier.shape = 1, outlier.size = 2) +
coord_flip() + labs(title = paste("Boxplot of", col), x = "Values", y = "") +
theme(plot.title = element_text(size = 8, hjust = 0.5))
# show the graphs
print(ggarrange(p1, p2, p3, nrow = 1, common.legend = FALSE))
}
# frequency tables
freq(dataframe[2])
# create the pie chart and bar graph for the Target column (DIAGNOSIS)
p1 <- ggplot(dataframe, aes(x = factor(1), fill = DIAGNOSIS)) + geom_bar() +
coord_polar("y") + labs(x = "DIAGNOSIS", y = "%")
p2 <- ggplot(dataframe, aes(x = factor(1), fill = DIAGNOSIS)) + geom_bar() +
labs(x = "DIAGNOSIS", y = "%")
# display the graphs
print(ggarrange(p1, p2, nrow = 1, ncol = 2, common.legend = TRUE))
# standarize
sca <- scale(datos[-1])
# transform data to large format
datos_largos <- as.data.frame(sca) %>%
rownames_to_column(var = "ID") %>%
pivot_longer(cols = -ID, names_to = "Variable", values_to = "Valor")
# reformat the graphical options
options(repr.plot.width = 10, repr.plot.height = 8)
# create boxplot
p <- ggplot(datos_largos, aes(x = Variable, y = Valor, fill = Variable)) +
geom_boxplot() +
theme(axis.text.x = element_text(angle = 90, hjust = 1)) +
labs(title = "Outliers", x = "All explanatory variables", y = "Values") +
scale_fill_discrete(name = "Variables")
# display the graphs
print(p)
columnas <- colnames(datos[-1])
# boxplot for each feature
for (col in columnas) {
p <- ggplot(datos[-1], aes_string(x = col)) +
geom_boxplot(outlier.colour = "red", outlier.shape = 1, outlier.size = 2) +
coord_flip() +
labs(title = paste("Boxplot of", col), x = "Values", y = "")
# show the graph
print(p)
}
# ---   OUTLIERS METHODOLOGY ---
# replacing outliers by the mean
reemplazar_outliers <- function(x) {
qnt <- quantile(x, probs=c(.25, .75), na.rm = TRUE)
H <- 1.5 * IQR(x, na.rm = TRUE)
outliers <- x < (qnt[1] - H) | x > (qnt[2] + H)
x[outliers] <- mean(x[!outliers])
return(x)
}
# we apply the function to every feature of our dataset
datos_preprocessed <- datos %>% mutate(across(-1, reemplazar_outliers))
# Uncomment to undo the outliers treatment
# datos_preprocessed = datos
columnas <- colnames(datos_preprocessed[-1])
# boxplot for each feature
for (col in columnas) {
p <- ggplot(datos_preprocessed[-1], aes_string(x = col)) +
geom_boxplot(outlier.colour = "red", outlier.shape = 1, outlier.size = 2) +
coord_flip() +
labs(title = paste("Boxplot of", col," (without outliers)"), x = "Values", y = "")
# show the graph
print(p)
}
columnas <- colnames(datos_preprocessed[-1])
# creating qqplot
for (col in columnas) {
p <- ggplot(data = datos_preprocessed, aes(sample = get(col))) +
stat_qq(distribution = qnorm, dparams = list(mean = mean(datos_preprocessed[[col]], na.rm = TRUE), sd = sd(datos_preprocessed[[col]], na.rm = TRUE))) +
geom_abline(color = "red", linetype = "dashed") +  # Agrega una línea diagonal
ggtitle(paste("Q-Q plot of", col)) +
xlab("Theoretical Quantiles") +
ylab("Sample Quantiles")
print(p)
}
columnas <- names(datos_preprocessed)[-1]
# Crea un histograma para cada característica
for (j0 in columnas) {
# Genera el histograma
p <- ggplot(datos_preprocessed, aes_string(x = j0)) +
geom_histogram(aes(y = ..density..), bins = 30, fill = "lightblue", color = "black") +
geom_density(color = "red", lwd = 1) +
labs(title = paste("Histogram of", j0), x = j0, y = "Density")
# Muestra el histograma
print(p)
}
#reformat the data
datos_tidy <- melt(datos_preprocessed, id.vars = "ID", variable.name = "variable", value.name = "value")
# Shapiro-Wilk test for each variable.
resultados <- aggregate(value ~ variable, data = datos_tidy, FUN = function(x){shapiro.test(x)$p.value})
# new column to find if the value obtained is greater than the p-value
resultados$NORMALITY <- ifelse(resultados$value < 0.05, "NO", "YES")
# display results
print(resultados)
###############################
# Correlation at sample level #
###############################
# Are the variables correlated at sample level?
correlation_matrix<-cor(datos_preprocessed[-1])
correlation_matrix
det(correlation_matrix)
cor(datos_preprocessed$perimeter1, datos_preprocessed$area1)
###################################
# Correlation at population level #
###################################
# Bartlett's sphericity test:
# This test checks wheter the correlations are significantly different from 0
# The null hypothesis is H_0; det(R)=1 means the variables are uncorrelated
# R denotes the correlation matrix
# cortest.bartlett function in the package pysch performs this test
# This function works with standarized data
#Standardization
datos_preprocessed_scale<-scale(datos_preprocessed[-1])
# Bartlett's sphericity test
cortest.bartlett(cor(datos_preprocessed_scale), 569) # 569 is the sample size
# The 'prcomp' function in the base R package performs this analysis
# Parameters 'scale' and 'center' are set to TRUE to consider standardized data
PCA<-prcomp(datos_preprocessed[-1], scale=T, center=T)
# The field 'rotation0 of the 'PCA' object is a matrix
# Its columns are the coefficients of the principal components
# Indicates the weight of each variable in the corresponding principal component
PCA$rotation
PCA$sdev
summary(PCA)
explained_variance<-PCA$sdev^2 / sum(PCA$sdev^2)
p1<-ggplot(data=data.frame(explained_variance, pc=1:ncol(datos_preprocessed[-1])),
aes(x= pc, y = explained_variance, fill=explained_variance)) + geom_col(width = 0.3) +
scale_y_continuous(limits = c(0,0.6)) + theme_bw() + labs(x = "Principal component", y = "Proportion of variance")
p1
cummulative_variance<-cumsum(explained_variance)
p2<-ggplot(data = data.frame(cummulative_variance, pc= 1:ncol(datos_preprocessed[-1])), aes(x = pc, y = cummulative_variance, fill=cummulative_variance)) + geom_col(width = 0.5) + scale_y_continuous(limits = c(0,1)) + theme_bw() + labs(x = "Principal component", y = "Proportion of cumulative variance")
p2
PCA$sdev^2
mean(PCA$sdev^2)
counter<-1
print("Prinicipal components considered:")
for(element in PCA$sdev^2){
if(element > mean(PCA$sdev^2)){
print(colnames(PCA$x)[counter])
}
counter<-counter + 1
}
p1<-fviz_pca_var(PCA,repel = T, col.var = "cos2", legend.title="Distance", title="Variables")+theme_bw()
p1
p1<-fviz_pca_ind(PCA, col.ind = "contrib", gradient.cols = c("#00AFBB", "#E7B800", "#FC4E07"),
repel = T, legend.title = "Contrib.var", title ="Recrods") + theme_bw()
p1
p1<-fviz_pca(PCA, alpha.ind = "contrib", col.var="cos2", col.ind="seagreen", gradient.cols = c("#FDF50E", "#FD960E", "#FD1E0E"), repel = T, legend.title = "Distancia") + theme_bw()
p1
# Polychoric correlation matrix
poly_cor<-hetcor(datos_preprocessed[-1])$correlations
ggcorrplot(poly_cor, type="lower", hc.order = T)
corrplot(cor(datos_preprocessed[-1]), order="hclust", tl.col="black", tl.cex = 1)
datos_preprocessed_correlations<-correlate(datos_preprocessed[-1]) #Correlations object
rplot(datos_preprocessed_correlations, legend = T, colours = c("firebrick1", "black", "darkcyan"), print_cor = T)
### Test of two models with three factors
model1<-fa(poly_cor, nfactors = 3, rotate = "none", fm="mle") #likhelihood method
model2<-fa(poly_cor, nfactors = 3, rotate = "none", fm = "minres") #minimal residual model
# Outputs of these models: factorial matrices, etc.
print("Model 1: maximum likelihood esthimator")
model1$loadings
print("Model 2: minimal residual model")
model2$loadings
# Comparing communalities
sort(model1$communality, decreasing = T)->c1
sort(model2$communality, decreasing = T)->c2
cbind(c1,c2)
sort(model1$uniquenesses, decreasing = T)->u1
sort(model2$uniquenesses, decreasing = T)->u2
cbind(u1,u2)
# Scree plot
scree(poly_cor)
#Parallel anaylisis
fa.parallel(poly_cor, n.obs=100, fa="fa", fm="minres")
factanal(datos, factors=5, rotation="none", lower=0.02)
varimax_model<-fa(poly_cor, nfactors=5, rotate="varimax", fa="minres")
varimax_model<-fa(poly_cor, nfactors=3, rotate="varimax", fa="minres")
factanal(datos, factors=3, rotation="none", lower=0.03)
print(varimax_model$loadings, cut = 0)
fa.diagram(varimax_model)
outliers <- mvn(data = datos[,-1], mvnTest = "hz", multivariateOutlierMethod = "quan")
royston_test <- mvn(data = datos_preprocessed[,-1], mvnTest = "royston", multivariatePlot = "qq")
royston_test$multivariateNormality
hz_test <- mvn(data = datos_preprocessed[,-1], mvnTest = "hz")
hz_test$multivariateNormality
unlink("Breast_Cancer_Analysis_cache", recursive = TRUE)
#########################################
# Loading necessary packages and reason #
#########################################
# This is an example of the first installation of a package
# Only runs once if the package is not installed
# Once it is installed this sentence has to be commented (not to run again)
#install.packages("ggplot2")
#install.packages("summarytools")
#install.packages("tidyverse")
#install.packages("pastecs")
# Package required to call 'freq' and 'descr' functions (descriptive statistics)
library(summarytools)
# Package required to call 'ggplot' function
library(ggplot2)
# Package required to call 'ggarrange' function
library(ggpubr)
# Package required to call 'scatterplot3d' function
library(scatterplot3d)
# Package required to call 'melt' function
library(reshape2)
# Package required to call 'mvn' function
library(MVN)
# Package required to call 'boxM' function
library(biotools)
# Package required to call 'summarise' function
library(dplyr)
# Package required to call 'createDataPartition' function
library(caret)
# Package required to call 'read.spss' function (loading '.spss' data format)
library(foreign)
# Package required to call 'read_xlsx' function (loading '.xlsx' data format)
library(readxl)
# Package required to load the data set 'RBGlass1'
library(archdata)
# Package required to call 'cortest.bartlett' function
library(psych)
# Package required to call 'fviz_pca_var, fviz_pca_ind and fviz_pca' functions
library(factoextra)
# Package required to call 'scatterplot3d' function
library(scatterplot3d)
# Package required to call 'factanal' function
library(stats)
# Package required to call 'freq' function
library(summarytools)
# Package required to call 'cortest.bartlett' function
library(psych)
# Package required to call 'hetcor' function
library(polycor)
# Package required to call 'ggcorrplot' function
library(ggcorrplot)
# Package required to call 'corrplot' function
library(corrplot)
# Package required to call 'rplot' function
library(corrr)
#  Package required to call 'ggplot' function
library(ggplot2)
library(ggpubr)
library(tidyverse)
#file path
#Valentin's path
#setwd("C:/Users/valentin/Desktop/DGIIM 5/1 CUATRI/MULTIVARIANTE/PRACTICAS/PRACTICA_FINAL/Breast_Cancer_Analysis")
#Javi's path
setwd("/home/javi5454/Desktop/Github/Breast_Cancer_Analysis")
#loading data. We haven seen that the file has the values of each feature separated by "," so we use as argument the specific separator ",". The file does not have header so we also specify it.
dataframe <- read.table("wdbc.data", header = FALSE, sep=",")
#verifying data.
head(dataframe)
#Columns' names from the dataset description
colnames<-c('ID','DIAGNOSIS','radius1', 'texture1','perimeter1','area1','smoothness1','compactness1','concavity1','concave_points1','symmetry1','fractal_dimension1','radius2','texture2','perimeter2','area2','smoothness2','compactness2','concavity2','concave_points2','symmetry2','fractal_dimension2','radius3','texture3','perimeter3','area3','smoothness3','compactness3','concavity3','concave_points3','symmetry3','fractal_dimension3')
#changing columns' names
colnames(dataframe)<- colnames
#erasing the first two columns
datos <- dataframe[, -c(1, 2)]
#finally we add an extra column to our dataframe which funtion is just identify each row, but regarding our order and not the dataframe's one.
datos$ID <- 1:nrow(datos)
datos <- datos[, c('ID', names(datos)[-ncol(datos)])]
#showing the data to verify the changes
datos
#discomment in case the file with the preprocessed dataframe is needed.
#write.table(datos, file = 'dataframe.csv', sep = ',', row.names = FALSE)
# compute the missing values for each feature of our dataset
missing_values <- sapply(datos, function(x) sum(is.na(x))/length(x) * 100)
# we plot the results of the missing values data
barplot(missing_values, main = "Porcentage of missing values", xlab = "", ylab = "Porcentage of missing values", col = "steelblue", las = 2, ylim = c(0, 100))
# for every feature except the ID
for(col in names(datos)[-1]) {
cat("\nSummary of the feature: ", col, "\n")
# showing the summary
print(summary(datos[[col]]))
}
#compute the measures for all features except the ID
breast_cancer_dataset <- datos[, -1]
descr_res <- descr(breast_cancer_dataset)
#show adequately the table
print((descr_res), method = 'render')
for(col in names(datos)[-1]) {
# creating density, histogram and boxplot
p1 <- ggplot(datos[-1], aes_string(x = col)) + geom_density() +
labs(title = paste("Density function of", col), x = col, y = "Values") +
theme(plot.title = element_text(size = 8, hjust = 0.5))
p2 <- ggplot(datos[-1], aes_string(x = col)) + geom_histogram(bins = 30) +
labs(title = paste("Histogram of", col), x = col, y = "Values") +
theme(plot.title = element_text(size = 8, hjust = 0.5))
p3 <- ggplot(datos[-1], aes_string(x = col)) +
geom_boxplot(outlier.colour = "red", outlier.shape = 1, outlier.size = 2) +
coord_flip() + labs(title = paste("Boxplot of", col), x = "Values", y = "") +
theme(plot.title = element_text(size = 8, hjust = 0.5))
# show the graphs
print(ggarrange(p1, p2, p3, nrow = 1, common.legend = FALSE))
}
# frequency tables
freq(dataframe[2])
# create the pie chart and bar graph for the Target column (DIAGNOSIS)
p1 <- ggplot(dataframe, aes(x = factor(1), fill = DIAGNOSIS)) + geom_bar() +
coord_polar("y") + labs(x = "DIAGNOSIS", y = "%")
p2 <- ggplot(dataframe, aes(x = factor(1), fill = DIAGNOSIS)) + geom_bar() +
labs(x = "DIAGNOSIS", y = "%")
# display the graphs
print(ggarrange(p1, p2, nrow = 1, ncol = 2, common.legend = TRUE))
# standarize
sca <- scale(datos[-1])
# transform data to large format
datos_largos <- as.data.frame(sca) %>%
rownames_to_column(var = "ID") %>%
pivot_longer(cols = -ID, names_to = "Variable", values_to = "Valor")
# reformat the graphical options
options(repr.plot.width = 10, repr.plot.height = 8)
# create boxplot
p <- ggplot(datos_largos, aes(x = Variable, y = Valor, fill = Variable)) +
geom_boxplot() +
theme(axis.text.x = element_text(angle = 90, hjust = 1)) +
labs(title = "Outliers", x = "All explanatory variables", y = "Values") +
scale_fill_discrete(name = "Variables")
# display the graphs
print(p)
columnas <- colnames(datos[-1])
# boxplot for each feature
for (col in columnas) {
p <- ggplot(datos[-1], aes_string(x = col)) +
geom_boxplot(outlier.colour = "red", outlier.shape = 1, outlier.size = 2) +
coord_flip() +
labs(title = paste("Boxplot of", col), x = "Values", y = "")
# show the graph
print(p)
}
# ---   OUTLIERS METHODOLOGY ---
# replacing outliers by the mean
reemplazar_outliers <- function(x) {
qnt <- quantile(x, probs=c(.25, .75), na.rm = TRUE)
H <- 1.5 * IQR(x, na.rm = TRUE)
outliers <- x < (qnt[1] - H) | x > (qnt[2] + H)
x[outliers] <- mean(x[!outliers])
return(x)
}
# we apply the function to every feature of our dataset
datos_preprocessed <- datos %>% mutate(across(-1, reemplazar_outliers))
# Uncomment to undo the outliers treatment
# datos_preprocessed = datos
columnas <- colnames(datos_preprocessed[-1])
# boxplot for each feature
for (col in columnas) {
p <- ggplot(datos_preprocessed[-1], aes_string(x = col)) +
geom_boxplot(outlier.colour = "red", outlier.shape = 1, outlier.size = 2) +
coord_flip() +
labs(title = paste("Boxplot of", col," (without outliers)"), x = "Values", y = "")
# show the graph
print(p)
}
columnas <- colnames(datos_preprocessed[-1])
# creating qqplot
for (col in columnas) {
p <- ggplot(data = datos_preprocessed, aes(sample = get(col))) +
stat_qq(distribution = qnorm, dparams = list(mean = mean(datos_preprocessed[[col]], na.rm = TRUE), sd = sd(datos_preprocessed[[col]], na.rm = TRUE))) +
geom_abline(color = "red", linetype = "dashed") +  # Agrega una línea diagonal
ggtitle(paste("Q-Q plot of", col)) +
xlab("Theoretical Quantiles") +
ylab("Sample Quantiles")
print(p)
}
columnas <- names(datos_preprocessed)[-1]
# Crea un histograma para cada característica
for (j0 in columnas) {
# Genera el histograma
p <- ggplot(datos_preprocessed, aes_string(x = j0)) +
geom_histogram(aes(y = ..density..), bins = 30, fill = "lightblue", color = "black") +
geom_density(color = "red", lwd = 1) +
labs(title = paste("Histogram of", j0), x = j0, y = "Density")
# Muestra el histograma
print(p)
}
#reformat the data
datos_tidy <- melt(datos_preprocessed, id.vars = "ID", variable.name = "variable", value.name = "value")
# Shapiro-Wilk test for each variable.
resultados <- aggregate(value ~ variable, data = datos_tidy, FUN = function(x){shapiro.test(x)$p.value})
# new column to find if the value obtained is greater than the p-value
resultados$NORMALITY <- ifelse(resultados$value < 0.05, "NO", "YES")
# display results
print(resultados)
###############################
# Correlation at sample level #
###############################
# Are the variables correlated at sample level?
correlation_matrix<-cor(datos_preprocessed[-1])
correlation_matrix
det(correlation_matrix)
cor(datos_preprocessed$perimeter1, datos_preprocessed$area1)
###################################
# Correlation at population level #
###################################
# Bartlett's sphericity test:
# This test checks wheter the correlations are significantly different from 0
# The null hypothesis is H_0; det(R)=1 means the variables are uncorrelated
# R denotes the correlation matrix
# cortest.bartlett function in the package pysch performs this test
# This function works with standarized data
#Standardization
datos_preprocessed_scale<-scale(datos_preprocessed[-1])
# Bartlett's sphericity test
cortest.bartlett(cor(datos_preprocessed_scale), 569) # 569 is the sample size
# The 'prcomp' function in the base R package performs this analysis
# Parameters 'scale' and 'center' are set to TRUE to consider standardized data
PCA<-prcomp(datos_preprocessed[-1], scale=T, center=T)
# The field 'rotation0 of the 'PCA' object is a matrix
# Its columns are the coefficients of the principal components
# Indicates the weight of each variable in the corresponding principal component
PCA$rotation
PCA$sdev
summary(PCA)
explained_variance<-PCA$sdev^2 / sum(PCA$sdev^2)
p1<-ggplot(data=data.frame(explained_variance, pc=1:ncol(datos_preprocessed[-1])),
aes(x= pc, y = explained_variance, fill=explained_variance)) + geom_col(width = 0.3) +
scale_y_continuous(limits = c(0,0.6)) + theme_bw() + labs(x = "Principal component", y = "Proportion of variance")
p1
cummulative_variance<-cumsum(explained_variance)
p2<-ggplot(data = data.frame(cummulative_variance, pc= 1:ncol(datos_preprocessed[-1])), aes(x = pc, y = cummulative_variance, fill=cummulative_variance)) + geom_col(width = 0.5) + scale_y_continuous(limits = c(0,1)) + theme_bw() + labs(x = "Principal component", y = "Proportion of cumulative variance")
p2
PCA$sdev^2
mean(PCA$sdev^2)
counter<-1
print("Prinicipal components considered:")
for(element in PCA$sdev^2){
if(element > mean(PCA$sdev^2)){
print(colnames(PCA$x)[counter])
}
counter<-counter + 1
}
p1<-fviz_pca_var(PCA,repel = T, col.var = "cos2", legend.title="Distance", title="Variables")+theme_bw()
p1
p1<-fviz_pca_ind(PCA, col.ind = "contrib", gradient.cols = c("#00AFBB", "#E7B800", "#FC4E07"),
repel = T, legend.title = "Contrib.var", title ="Recrods") + theme_bw()
p1
p1<-fviz_pca(PCA, alpha.ind = "contrib", col.var="cos2", col.ind="seagreen", gradient.cols = c("#FDF50E", "#FD960E", "#FD1E0E"), repel = T, legend.title = "Distancia") + theme_bw()
p1
# Polychoric correlation matrix
poly_cor<-hetcor(datos_preprocessed[-1])$correlations
ggcorrplot(poly_cor, type="lower", hc.order = T)
corrplot(cor(datos_preprocessed[-1]), order="hclust", tl.col="black", tl.cex = 1)
datos_preprocessed_correlations<-correlate(datos_preprocessed[-1]) #Correlations object
rplot(datos_preprocessed_correlations, legend = T, colours = c("firebrick1", "black", "darkcyan"), print_cor = T)
### Test of two models with three factors
model1<-fa(poly_cor, nfactors = 3, rotate = "none", fm="mle") #likhelihood method
model2<-fa(poly_cor, nfactors = 3, rotate = "none", fm = "minres") #minimal residual model
# Outputs of these models: factorial matrices, etc.
print("Model 1: maximum likelihood esthimator")
model1$loadings
print("Model 2: minimal residual model")
model2$loadings
# Comparing communalities
sort(model1$communality, decreasing = T)->c1
sort(model2$communality, decreasing = T)->c2
cbind(c1,c2)
sort(model1$uniquenesses, decreasing = T)->u1
sort(model2$uniquenesses, decreasing = T)->u2
cbind(u1,u2)
# Scree plot
scree(poly_cor)
#Parallel anaylisis
fa.parallel(poly_cor, n.obs=100, fa="fa", fm="minres")
factanal(datos, factors=5, rotation="none", lower=0.02)
varimax_model<-fa(poly_cor, nfactors=5, rotate="varimax", fa="minres")
varimax_model<-fa(poly_cor, nfactors=3, rotate="varimax", fa="minres")
factanal(datos, factors=3, rotation="none", lower=0.03)
print(varimax_model$loadings, cut = 0)
fa.diagram(varimax_model)
outliers <- mvn(data = datos[,-1], mvnTest = "hz", multivariateOutlierMethod = "quan")
royston_test <- mvn(data = datos_preprocessed[,-1], mvnTest = "royston", multivariatePlot = "qq")
royston_test$multivariateNormality
hz_test <- mvn(data = datos_preprocessed[,-1], mvnTest = "hz")
hz_test$multivariateNormality
#compute the measures for all features except the ID
breast_cancer_dataset <- datos[, -1]
descr_res <- descr(breast_cancer_dataset, out="txt")
#show adequately the table
print((descr_res), method = 'render')
#compute the measures for all features except the ID
breast_cancer_dataset <- datos[, -1]
descr_res <- descr(breast_cancer_dataset)
#show adequately the table
print((descr_res), method = 'render')
#compute the measures for all features except the ID
breast_cancer_dataset <- datos[, -1]
descr_res <- descr(breast_cancer_dataset)
#show adequately the table
print((descr_res), method = 'render')
#compute the measures for all features except the ID
breast_cancer_dataset <- datos[, -1]
descr_res <- descr(breast_cancer_dataset)
#show adequately the table
print((descr_res), method = 'render')
